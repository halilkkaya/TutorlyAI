from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import os
from typing import Optional, AsyncGenerator, List, Dict, Any
import json
import asyncio
from dotenv import load_dotenv
import traceback
import re
# fal-client kÃ¼tÃ¼phanesini iÃ§e aktar
import fal_client

# RAG sistemi iÃ§in import'lar
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document
import chromadb
from PyPDF2 import PdfReader
import glob
from pathlib import Path

# Ortam deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()

SUBJECT_ALIASES = {
    "din": "din",
    "din_kÃ¼ltÃ¼rÃ¼": "din",
    "din_kulturu": "din",
    "inkilap": "inkilap",
    "inkÄ±lap": "inkilap",
    "cografya": "cografya",
    "coÄŸrafya": "cografya",
    "turk_dili_ve_edebiyati": "turkdili",
    "turk_dili_ve_edebiyatÄ±": "turkdili",
    "turkce": "turkce",
    "tÃ¼rkÃ§e": "turkce",
    "biyoloji": "biyoloji",
    "fizik": "fizik",
    "kimya": "kimya",
    "matematik": "matematik",
    "tarih": "tarih"
}

def canonical_subject(raw: str) -> str:
    s = raw.lower().strip()
    s = re.sub(r'[_\s\-]+', '_', s)
    return SUBJECT_ALIASES.get(s, s)


# FastAPI uygulamasÄ± oluÅŸtur
app = FastAPI(
    title="Fal.ai Any-LLM (Gemini 2.5 Flash) API with Enhanced RAG",
    description="GeliÅŸmiÅŸ RAG sistemi ile fal.ai 'any-llm' modeli kullanarak streaming metin Ã¼retme API'si",
    version="2.0.0"
)

# CORS ayarlarÄ±
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic model - istek yapÄ±sÄ±
class TextGenerationRequest(BaseModel):
    prompt: str
    max_tokens: Optional[int] = 1000
    temperature: Optional[float] = 0.7
    tools: Optional[List[Dict[str, Any]]] = None
    system_prompt: Optional[str] = None

# RAG sistemi iÃ§in yanÄ±t modeli
class ToolResponse(BaseModel):
    tool_call_id: str
    result: str

# Fal.ai API anahtarÄ±nÄ± kontrol et
FAL_KEY = os.getenv("FAL_KEY")

if not FAL_KEY:
    raise ValueError("FAL_KEY ortam deÄŸiÅŸkeni ayarlanmamÄ±ÅŸ!")

# Model adÄ±nÄ± belirle
MODEL_NAME = "google/gemini-2.5-flash"
FAL_MODEL_GATEWAY = "fal-ai/any-llm"

# RAG sistemi iÃ§in global deÄŸiÅŸkenler
vectorstore = None
embedding_model = None
text_splitter = None
_PAT_FULL = re.compile(
    r"^(?P<grade>\d{1,2})_sinif_(?P<subject>[a-z0-9_]+)_unite_(?P<unit>\d{1,2})_(?P<slug>[a-z0-9_]+)\.pdf$",
    re.IGNORECASE
)

def parse_filename_for_metadata(filename: str):
    """
    Beklenen format:
    <grade>_sinif_<subject>_unite_<unit>_<slug>.pdf
    Ã–rn: 9_sinif_biyoloji_unite_01_yasam.pdf
         9_sinif_din_unite_2_islamda_inanc_esaslari.pdf
    """
    name = filename
    if name.lower().endswith(".pdf"):
        name = name[:-4]

    m = _PAT_FULL.match(filename)
    if not m:
        # Uymayan dosyalarÄ± sessizce geÃ§mek yerine logla:
        print(f"[METADATA] UYUMSUZ AD: {filename}")
        return None

    grade = int(m.group("grade"))
    subject = canonical_subject(m.group("subject"))
    unit = int(m.group("unit"))
    slug = m.group("slug").lower().strip("_")

    meta = {
        "sinif": grade,
        "ders": subject,
        "unite": unit,
        "konu_slug": slug
    }
    print(f"[METADATA] AyrÄ±ÅŸtÄ±rÄ±ldÄ±: {filename} -> {meta}")
    return meta

# Arama planlayÄ±cÄ±sÄ± iÃ§in geliÅŸmiÅŸ system prompt
QUERY_PLANNER_SYSTEM_PROMPT = """Sen bir akÄ±llÄ± arama asistanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n sorusunu analiz ederek vektÃ¶r arama iÃ§in optimal parametreleri oluÅŸturacaksÄ±n.

Ã‡Ä±ktÄ±n SADECE aÅŸaÄŸÄ±daki JSON formatÄ±nda olmalÄ±dÄ±r:
{
  "query": "anahtar kelimeler ve kavramlar",
  "filters": {
    "sinif": 9,
    "ders": "biyoloji",
    "unite": 1,
    "konu_slug": "yasam"
  }
}

KURALLAR:
1. "query" alanÄ±nda Ã¶nemli kavramlarÄ± ve anahtar kelimeleri ayÄ±kla
2. Varsa sÄ±nÄ±f (9/10/11/12), ders (turkce, matematik, kimya, biyoloji, fizik, tarih, cografya, din, turkdili), Ã¼nite (tamsayÄ±), konu_slug (kÄ±sa, alt Ã§izgili) bilgilerini "filters" iÃ§ine ekle
3. Ders adlarÄ±nÄ± kÃ¼Ã§Ã¼k harfle ve kanonik yaz: "din", "cografya", "turkce", "inkilap" gibi
4. SÄ±nÄ±f mutlaka 9, 10, 11 veya 12 olmalÄ±dÄ±r; belirsizse bu alanÄ± yazma
5. KullanÄ±cÄ± Ã¼nite/konu belirtmiÅŸse "unite" (int) ve "konu_slug" (kÄ±sa slug) eklemeye Ã§alÄ±ÅŸ
6. EÄŸer filtre bilgisi yoksa filters={} bÄ±rak

Ã–RNEKLER:

"10. sÄ±nÄ±f biyoloji hÃ¼cre bÃ¶lÃ¼nmesi nedir?" â†’ 
{
  "query": "hÃ¼cre bÃ¶lÃ¼nmesi mitoz mayoz",
  "filters": {"sinif": 10, "ders": "biyoloji"}
}

"9. sÄ±nÄ±f kimya Ã¼nite 1: etkileÅŸim Ã¶rnekleri" â†’
{
  "query": "kimyasal etkileÅŸim Ã¶rnekleri baÄŸ tÃ¼rleri",
  "filters": {"sinif": 9, "ders": "kimya", "unite": 1, "konu_slug": "etkilesim"}
}

"din kÃ¼ltÃ¼rÃ¼ islamda inanÃ§ esaslarÄ± aÃ§Ä±klama" â†’
{
  "query": "islamda inanÃ§ esaslarÄ± iman ÅŸartlarÄ±",
  "filters": {"ders": "din", "konu_slug": "islamda_inanc_esaslari"}
}
"""

async def get_search_plan(user_prompt: str) -> Dict[str, Any]:
    """KullanÄ±cÄ± sorgusundan arama planÄ± oluÅŸturur"""
    print(f"[PLANNER] Sorgu analizi: '{user_prompt}'")
    
    try:
        result = await fal_client.run_async(
            FAL_MODEL_GATEWAY,
            arguments={
                "model": MODEL_NAME,
                "prompt": user_prompt,
                "system_prompt": QUERY_PLANNER_SYSTEM_PROMPT,
                "max_tokens": 150,
                "temperature": 0.1,
            },
        )
        
        response_text = result.get("output", "{}").strip()
        print(f"[PLANNER] Model yanÄ±tÄ±: {response_text}")
        
        # JSON'u Ã§Ä±kar
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
        
        if not json_match:
            print("[PLANNER] JSON bulunamadÄ±, varsayÄ±lan plan kullanÄ±lÄ±yor")
            return {"query": user_prompt, "filters": {}}
        
        clean_json = json_match.group(0)
        plan = json.loads(clean_json)
        
        # Plan doÄŸrulama
        if "query" not in plan:
            plan["query"] = user_prompt
        if "filters" not in plan:
            plan["filters"] = {}
        
        # BoÅŸ filtreleri temizle
        plan["filters"] = {k: v for k, v in plan["filters"].items() 
                          if v is not None and str(v).strip() != ""}
        
        print(f"[PLANNER] Final plan: {plan}")
        return plan
        
    except Exception as e:
        print(f"[PLANNER] Hata: {e}")
        return {"query": user_prompt, "filters": {}}

@app.get("/")
async def root():
    """Ana endpoint"""
    global vectorstore
    
    # Veri sayÄ±sÄ±nÄ± kontrol et
    doc_count = 0
    if vectorstore:
        try:
            doc_count = vectorstore._collection.count()
        except:
            doc_count = -1
    
    return {
        "message": "GeliÅŸmiÅŸ RAG Sistemi Aktif!",
        "status": "active",
        "version": "2.0.0",
        "documents_loaded": doc_count,
        "features": [
            "AkÄ±llÄ± arama planlayÄ±cÄ±sÄ±",
            "Metadata tabanlÄ± filtreleme",
            "9-12. sÄ±nÄ±f ders kitaplarÄ±",
            "GeliÅŸmiÅŸ PDF okuma",
            "Debug endpoint'leri"
        ],
        "endpoints": {
            "generate": "/generate",
            "generate_stream": "/generate/stream", 
            "load_books": "/load-books",
            "search": "/search",
            "debug": "/debug",
            "rag_status": "/rag-status"
        }
    }

@app.get("/health")
async def health_check():
    """SaÄŸlÄ±k kontrolÃ¼"""
    return {"status": "healthy", "rag_active": vectorstore is not None}

@app.post("/load-books")
async def load_books_endpoint():
    """KitaplarÄ± yÃ¼kler"""
    try:
        success = await load_books_async()
        
        if success:
            return {
                "message": "Kitaplar baÅŸarÄ±yla yÃ¼klendi",
                "status": "success"
            }
        else:
            return {
                "message": "Kitap yÃ¼kleme baÅŸarÄ±sÄ±z",
                "status": "failed"
            }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Kitap yÃ¼kleme hatasÄ±: {str(e)}")

@app.get("/rag-status")
async def rag_status():
    """RAG sistemi durumu"""
    global vectorstore
    
    if not vectorstore:
        return {"status": "inactive", "message": "RAG sistemi baÅŸlatÄ±lmamÄ±ÅŸ"}
    
    try:
        collection = vectorstore._collection
        count = collection.count()
        
        # Metadata Ã¶rnekleri al
        results = collection.peek(limit=5)
        sample_metadata = []
        if results and 'metadatas' in results:
            sample_metadata = results['metadatas']
        
        return {
            "status": "active",
            "documents_count": count,
            "embedding_model": "paraphrase-multilingual-MiniLM-L12-v2",
            "vectorstore": "ChromaDB",
            "sample_metadata": sample_metadata[:3]  # Ä°lk 3 Ã¶rnek
        }
    except Exception as e:
        return {"status": "error", "error": str(e)}

@app.post("/search")
async def search_endpoint(request: dict):
    """Manuel arama endpoint'i"""
    query = request.get("query", "")
    filters = request.get("filters", {})
    k = request.get("k", 5)
    
    if not query:
        raise HTTPException(status_code=400, detail="Query gerekli")
    
    results = search_books_enhanced(query, filters, k)
    
    return {
        "query": query,
        "filters": filters,
        "results_count": len(results),
        "results": [
            {
                "content": doc.page_content[:200] + "...",
                "metadata": doc.metadata,
                "relevance_score": getattr(doc, 'relevance_score', None)
            }
            for doc in results
        ]
    }

@app.get("/debug")
async def debug_endpoint():
    """Debug bilgileri"""
    global vectorstore
    
    if not vectorstore:
        return {"error": "RAG sistemi aktif deÄŸil"}
    
    try:
        collection = vectorstore._collection
        
        # Toplam dokÃ¼man sayÄ±sÄ±
        total_docs = collection.count()
        
        # Metadata Ã¶rnekleri
        peek_result = collection.peek(limit=10)
        
        # SÄ±nÄ±f ve ders daÄŸÄ±lÄ±mÄ±nÄ± hesapla
        all_metadata = peek_result.get('metadatas', [])
        
        class_dist = {}
        subject_dist = {}
        
        for meta in all_metadata:
            if 'sinif' in meta:
                class_num = meta['sinif']
                class_dist[class_num] = class_dist.get(class_num, 0) + 1
            
            if 'ders' in meta:
                subject = meta['ders']
                subject_dist[subject] = subject_dist.get(subject, 0) + 1
        
        return {
            "total_documents": total_docs,
            "class_distribution": class_dist,
            "subject_distribution": subject_dist,
            "sample_metadata": all_metadata[:5],
            "books_directory_files": len(glob.glob("books/*.pdf"))
        }
        
    except Exception as e:
        return {"error": str(e)}


async def generate_stream(request: TextGenerationRequest) -> AsyncGenerator[str, None]:
    """RAG entegreli streaming metin Ã¼retimi"""
    try:
        # ðŸ” 1. ADIM: RAG AramasÄ± (hÄ±zlÄ±)
        print(f"[STREAM] RAG aramasÄ± baÅŸlatÄ±lÄ±yor: '{request.prompt}'")
        
        # Arama durumunu kullanÄ±cÄ±ya bildir
        status_data = {
            "status": "searching",
            "message": "Ders kitaplarÄ±nda arama yapÄ±lÄ±yor...",
            "done": False
        }
        yield f"data: {json.dumps(status_data)}\n\n"
        
        # Arama planÄ± oluÅŸtur
        search_plan = await get_search_plan(request.prompt)
        query = search_plan.get("query", request.prompt)
        filters = search_plan.get("filters", {})
        
        # KitaplarÄ± ara
        relevant_docs = search_books_enhanced(query, filters, k=4)
        
        # Arama sonucunu bildir
        search_result_data = {
            "status": "search_complete",
            "message": f"{len(relevant_docs)} kaynak bulundu, cevap oluÅŸturuluyor...",
            "found_sources": len(relevant_docs),
            "search_plan": search_plan,
            "done": False
        }
        yield f"data: {json.dumps(search_result_data)}\n\n"
        
        # ðŸ“š 2. ADIM: Context HazÄ±rlama
        if relevant_docs:
            context_text = "\n\n---\n\n".join([
                f"[{doc.metadata.get('source', 'Kaynak')}]\n{doc.page_content}"
                for doc in relevant_docs
            ])
            
            # Enhanced prompt with context
            enhanced_prompt = f"""Sen bir ders kitabÄ± uzmanÄ±sÄ±n. AÅŸaÄŸÄ±daki soruyu, verilen ders kitabÄ± metinlerini kullanarak cevapla.

SORU: {request.prompt}

DERS KÄ°TABI Ä°Ã‡ERÄ°ÄžÄ°:
{context_text}

KURALLAR:
1. Sadece verilen kaynaklardaki bilgileri kullan
2. KapsamlÄ± ve anlaÅŸÄ±lÄ±r bir aÃ§Ä±klama yap  
3. Hangi kaynaktan bilgi aldÄ±ÄŸÄ±nÄ± belirt
4. CevabÄ± direkt ver, giriÅŸ yapma

CEVAP:"""

            sources = [doc.metadata.get('source', 'Bilinmeyen') for doc in relevant_docs]
            
        else:
            # EÄŸer kaynak bulunamazsa genel bilgi ile devam et
            enhanced_prompt = f"""Soru: {request.prompt}

Bu soruyla ilgili ders kitaplarÄ±nda spesifik bilgi bulamadÄ±m, ama genel bilgilerimi kullanarak yardÄ±mcÄ± olmaya Ã§alÄ±ÅŸacaÄŸÄ±m:"""
            sources = []
        
        # ðŸš€ 3. ADIM: Streaming baÅŸlat
        print(f"[STREAM] LLM streaming baÅŸlatÄ±lÄ±yor...")
        
        # Fal.ai stream_async kullan
        stream = fal_client.stream_async(
            FAL_MODEL_GATEWAY,
            arguments={
                "model": MODEL_NAME,
                "prompt": enhanced_prompt,
                "max_tokens": request.max_tokens,
                "temperature": request.temperature,
            },
        )

        # Ä°lk token geldiÄŸinde generation baÅŸladÄ±ÄŸÄ±nÄ± bildir
        first_token = True
        
        # Stream'i dinle
        async for event in stream:
            # Event yapÄ±sÄ±nÄ± logla (debug iÃ§in)
            print(f"[STREAM DEBUG] Event: {event}")
            
            # FarklÄ± event tiplerini kontrol et
            if isinstance(event, dict):
                # Text chunk event'i
                if "chunk" in event and event["chunk"]:
                    chunk_text = event["chunk"]
                    
                    if first_token:
                        # Ä°lk token geldiÄŸinde generation_started event'i gÃ¶nder
                        generation_start_data = {
                            "status": "generation_started",
                            "message": "Cevap oluÅŸturuluyor...",
                            "sources": sources,
                            "done": False
                        }
                        yield f"data: {json.dumps(generation_start_data)}\n\n"
                        first_token = False
                    
                    # Normal text chunk
                    chunk_data = {
                        "text": chunk_text,
                        "type": "content",
                        "done": False
                    }
                    yield f"data: {json.dumps(chunk_data)}\n\n"
                
                # Alternative formats
                elif "output" in event and event["output"]:
                    chunk_text = event["output"]
                    
                    if first_token:
                        generation_start_data = {
                            "status": "generation_started", 
                            "message": "Cevap oluÅŸturuluyor...",
                            "sources": sources,
                            "done": False
                        }
                        yield f"data: {json.dumps(generation_start_data)}\n\n"
                        first_token = False
                    
                    chunk_data = {
                        "text": chunk_text,
                        "type": "content",
                        "done": False
                    }
                    yield f"data: {json.dumps(chunk_data)}\n\n"
                
                # Stream completed
                elif "done" in event or "response" in event:
                    final_response = event.get("response", "")
                    
                    # Generation tamamlandÄ±
                    final_data = {
                        "status": "completed",
                        "message": "Cevap tamamlandÄ±",
                        "full_response": final_response,
                        "sources": sources,
                        "search_plan": search_plan,
                        "found_documents": len(relevant_docs),
                        "done": True
                    }
                    yield f"data: {json.dumps(final_data)}\n\n"
                    break
                
                # Error handling
                elif "error" in event:
                    error_data = {
                        "status": "error",
                        "error": str(event["error"]),
                        "message": "Stream sÄ±rasÄ±nda hata oluÅŸtu",
                        "done": True
                    }
                    yield f"event: error\ndata: {json.dumps(error_data)}\n\n"
                    break
            
            # String event (simple text)
            elif isinstance(event, str) and event.strip():
                if first_token:
                    generation_start_data = {
                        "status": "generation_started",
                        "message": "Cevap oluÅŸturuluyor...",
                        "sources": sources,
                        "done": False
                    }
                    yield f"data: {json.dumps(generation_start_data)}\n\n"
                    first_token = False
                
                chunk_data = {
                    "text": event,
                    "type": "content", 
                    "done": False
                }
                yield f"data: {json.dumps(chunk_data)}\n\n"

    except Exception as e:
        print(f"[STREAM ERROR] {str(e)}")
        traceback.print_exc()
        error_data = {
            "status": "error",
            "error": str(e),
            "message": "Bir hata oluÅŸtu",
            "done": True
        }
        yield f"event: error\ndata: {json.dumps(error_data)}\n\n"

@app.post("/generate/stream")
async def stream_text(request: TextGenerationRequest):
    """Streaming metin Ã¼retme"""
    return StreamingResponse(
        generate_stream(request),
        media_type="text/event-stream"
    )

@app.post("/generate")
async def generate_rag_answer(request: TextGenerationRequest):
    """RAG ile cevap Ã¼retir"""
    try:
        print(f"[GENERATE] Gelen sorgu: '{request.prompt}'")
        
        # 1. Arama planÄ± oluÅŸtur
        search_plan = await get_search_plan(request.prompt)
        query = search_plan.get("query", request.prompt)
        filters = search_plan.get("filters", {})
        
        print(f"[GENERATE] Arama planÄ± - Query: '{query}', Filters: {filters}")
        
        # 2. KitaplarÄ± ara
        relevant_docs = search_books_enhanced(query, filters, k=5)
        
        if not relevant_docs:
            print("[GENERATE] HiÃ§ dokÃ¼man bulunamadÄ±")
            return {
                "generated_text": "ÃœzgÃ¼nÃ¼m, sorunuzla ilgili ders kitaplarÄ±nda bilgi bulamadÄ±m. Sorunuzu farklÄ± kelimelerle tekrar sorabilir misiniz?",
                "search_plan": search_plan,
                "found_documents": 0
            }
        
        # 3. Bulunan dÃ¶kÃ¼manlarÄ± birleÅŸtir
        context_text = "\n\n---\n\n".join([
            f"[{doc.metadata.get('source', 'Bilinmeyen')}]\n{doc.page_content}"
            for doc in relevant_docs
        ])
        
        print(f"[GENERATE] {len(relevant_docs)} dokÃ¼man bulundu, context oluÅŸturuluyor...")
        
        # 4. Nihai cevap oluÅŸtur
        synthesis_prompt = f"""Sen bir ders kitabÄ± uzmanÄ±sÄ±n. AÅŸaÄŸÄ±daki soruyu, verilen ders kitabÄ± metinlerini kullanarak cevapla.

SORU: {request.prompt}

DERS KÄ°TABI Ä°Ã‡ERÄ°ÄžÄ°:
{context_text}

KURALLAR:
1. Sadece verilen kaynaklardaki bilgileri kullan
2. KapsamlÄ± ve anlaÅŸÄ±lÄ±r bir aÃ§Ä±klama yap  
3. Hangi kaynaktan bilgi aldÄ±ÄŸÄ±nÄ± belirt
4. EÄŸer cevap kaynaklarda yoksa, bunu sÃ¶yle

CEVAP:"""

        result = await fal_client.run_async(
            FAL_MODEL_GATEWAY,
            arguments={
                "model": MODEL_NAME,
                "prompt": synthesis_prompt,
                "max_tokens": 1500,
                "temperature": 0.3,
            },
        )
        
        final_answer = result.get("output", "Cevap oluÅŸturulamadÄ±.")
        
        return {
            "generated_text": final_answer,
            "search_plan": search_plan,
            "found_documents": len(relevant_docs),
            "sources": [doc.metadata.get('source', 'Bilinmeyen') for doc in relevant_docs]
        }
        
    except Exception as e:
        print(f"[GENERATE ERROR] {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
async def get_model_info():
    """Model bilgileri"""
    return {
        "service": "fal.ai",
        "gateway_model": FAL_MODEL_GATEWAY,
        "configured_llm": MODEL_NAME
    }

# RAG sistem fonksiyonlarÄ±
def initialize_rag_system():
    """RAG sistemini baÅŸlatÄ±r"""
    global vectorstore, embedding_model, text_splitter
    
    try:
        print("[RAG] Sistem baÅŸlatÄ±lÄ±yor...")
        
        # Embedding modeli
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("[RAG] Embedding model yÃ¼klendi")
        
        # Text splitter
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,      # Daha kÃ¼Ã§Ã¼k chunk'lar
            chunk_overlap=100,   # Daha az overlap
            length_function=len,
            separators=["\n\n", "\n", ". ", ", ", " ", ""]
        )
        print("[RAG] Text splitter hazÄ±rlandÄ±")
        
        # ChromaDB
        vectorstore = Chroma(
            collection_name="enhanced_books_collection",
            embedding_function=embedding_model,
            persist_directory="./chroma_db_v2"  # Yeni versiyon iÃ§in farklÄ± klasÃ¶r
        )
        print("[RAG] ChromaDB baÄŸlandÄ±")
        
        return True
        
    except Exception as e:
        print(f"[RAG ERROR] BaÅŸlatma hatasÄ±: {str(e)}")
        return False

async def load_books_async():
    """KitaplarÄ± asenkron olarak yÃ¼kler"""
    global vectorstore, text_splitter
    
    if not vectorstore or not text_splitter:
        print("[RAG] Sistem baÅŸlatÄ±lmamÄ±ÅŸ")
        return False
    
    try:
        print("[RAG] Kitaplar yÃ¼kleniyor...")
        
        # PDF dosyalarÄ±nÄ± bul
        pdf_files = glob.glob("books/*.pdf")
        
        if not pdf_files:
            print("[RAG] books/ klasÃ¶rÃ¼nde PDF bulunamadÄ±")
            return False
        
        print(f"[RAG] {len(pdf_files)} PDF dosyasÄ± bulundu")
        
        all_documents = []
        successful_files = 0
        
        for pdf_path in pdf_files:
            try:
                filename = Path(pdf_path).name
                print(f"[RAG] Ä°ÅŸleniyor: {filename}")
                
                # Metadata Ã§Ä±kar
                metadata = parse_filename_for_metadata(filename)
                if not metadata:
                    print(f"[RAG] UYARI: {filename} format uyumsuz, atlanÄ±yor")
                    continue
                
                # PDF oku
                reader = PdfReader(pdf_path)
                full_text = ""
                
                print(f"[RAG] {filename}: {len(reader.pages)} sayfa okunuyor...")
                
                for page_num, page in enumerate(reader.pages):
                    try:
                        text = page.extract_text()
                        if text and text.strip():
                            # Metni temizle
                            text = re.sub(r'\s+', ' ', text).strip()
                            full_text += text + "\n"
                    except Exception as e:
                        print(f"[RAG] Sayfa {page_num} okuma hatasÄ±: {e}")
                        continue
                
                if not full_text.strip():
                    print(f"[RAG] UYARI: {filename} metin Ã§Ä±karÄ±lamadÄ±")
                    continue
                
                # Chunk'lara ayÄ±r
                chunks = text_splitter.split_text(full_text)
                print(f"[RAG] {filename}: {len(chunks)} chunk oluÅŸturuldu")
                
                # DÃ¶kÃ¼man objelerini oluÅŸtur
                for i, chunk in enumerate(chunks):
                    if len(chunk.strip()) > 50:  # Ã‡ok kÄ±sa chunk'larÄ± filtrele
                        doc_metadata = {
                            "source": Path(pdf_path).stem,
                            "filename": filename,
                            "chunk_id": i,
                            "chunk_length": len(chunk),
                            **metadata  # sinif ve ders bilgileri
                        }
                        
                        all_documents.append(Document(
                            page_content=chunk.strip(),
                            metadata=doc_metadata
                        ))
                
                successful_files += 1
                print(f"[RAG] {filename} baÅŸarÄ±yla iÅŸlendi")
                
            except Exception as e:
                print(f"[RAG] {pdf_path} iÅŸlenirken hata: {str(e)}")
                continue
        
        if all_documents:
            print(f"[RAG] {len(all_documents)} dokÃ¼man ChromaDB'ye ekleniyor...")
            
            # Batch olarak ekle (bÃ¼yÃ¼k dosyalar iÃ§in)
            batch_size = 100
            for i in range(0, len(all_documents), batch_size):
                batch = all_documents[i:i + batch_size]
                vectorstore.add_documents(batch)
                print(f"[RAG] Batch {i//batch_size + 1}/{(len(all_documents)-1)//batch_size + 1} eklendi")
            
            print(f"[RAG] BAÅžARILI: {successful_files} dosya, {len(all_documents)} chunk yÃ¼klendi")
            return True
        else:
            print("[RAG] HiÃ§ dokÃ¼man oluÅŸturulamadÄ±")
            return False
            
    except Exception as e:
        print(f"[RAG] Kitap yÃ¼kleme genel hatasÄ±: {str(e)}")
        traceback.print_exc()
        return False

def search_books_enhanced(query: str, filters: Optional[Dict[str, Any]] = None, k: int = 5) -> List[Document]:
    """GeliÅŸmiÅŸ kitap arama fonksiyonu"""
    global vectorstore
    
    if not vectorstore:
        print("[SEARCH] RAG sistemi aktif deÄŸil")
        return []
    
    try:
        print(f"[SEARCH] Arama baÅŸlatÄ±lÄ±yor: '{query}'")
        print(f"[SEARCH] Filtreler: {filters}")
        
        # Arama parametreleri
        search_kwargs = {"k": k}
        
        # ChromaDB filtreleme formatÄ±
        if filters:
            # Birden fazla filtre iÃ§in $and operatÃ¶rÃ¼
            if len(filters) > 1:
                where_clause = {
                    "$and": [
                        {key: {"$eq": value}} for key, value in filters.items()
                    ]
                }
            else:
                # Tek filtre
                key, value = list(filters.items())[0]
                where_clause = {key: {"$eq": value}}
            
            search_kwargs["filter"] = where_clause
            print(f"[SEARCH] ChromaDB filtre: {where_clause}")
        
        # Arama yap
        docs = vectorstore.similarity_search(query, **search_kwargs)
        
        print(f"[SEARCH] {len(docs)} sonuÃ§ bulundu")
        
        # SonuÃ§larÄ± logla
        for i, doc in enumerate(docs):
            print(f"[SEARCH] SonuÃ§ {i+1}: {doc.metadata.get('source', 'N/A')} - "
                  f"SÄ±nÄ±f: {doc.metadata.get('sinif', 'N/A')} - "
                  f"Ders: {doc.metadata.get('ders', 'N/A')} - "
                  f"Ä°Ã§erik: {doc.page_content[:100]}...")
        
        return docs
        
    except Exception as e:
        print(f"[SEARCH ERROR] Arama hatasÄ±: {str(e)}")
        traceback.print_exc()
        return []

def should_load_books():
    """KitaplarÄ±n yÃ¼klenip yÃ¼klenmediÄŸini kontrol eder"""
    global vectorstore
    
    if not vectorstore:
        return True
    
    try:
        doc_count = vectorstore._collection.count()
        pdf_count = len(glob.glob("books/*.pdf"))
        
        print(f"[RAG] ChromaDB'de {doc_count} dokÃ¼man var")
        print(f"[RAG] books/ klasÃ¶rÃ¼nde {pdf_count} PDF var")
        
        # EÄŸer hiÃ§ dokÃ¼man yoksa veya Ã§ok az varsa yÃ¼kle
        return doc_count < (pdf_count * 5)  # Her PDF en az 5 chunk Ã¼retmeli
        
    except Exception as e:
        print(f"[RAG] Kontrol hatasÄ±: {str(e)}")
        return True

if __name__ == "__main__":
    print("=" * 60)
    print("GeliÅŸmiÅŸ RAG Sistemi BaÅŸlatÄ±lÄ±yor")
    print("=" * 60)
    
    # RAG sistemini baÅŸlat
    if initialize_rag_system():
        print("[âœ“] RAG sistemi baÅŸlatÄ±ldÄ±")
        
        # KitaplarÄ± yÃ¼kle
        if should_load_books():
            print("[INFO] Kitaplar yÃ¼kleniyor...")
            import asyncio
            success = asyncio.run(load_books_async())
            
            if success:
                print("[âœ“] Kitaplar baÅŸarÄ±yla yÃ¼klendi")
            else:
                print("[!] Kitap yÃ¼kleme baÅŸarÄ±sÄ±z")
        else:
            print("[INFO] Kitaplar zaten yÃ¼klÃ¼")
    else:
        print("[âœ—] RAG sistemi baÅŸlatÄ±lamadÄ±")
    
    print("=" * 60)
    print(f"Model: {MODEL_NAME}")
    print("API baÅŸlatÄ±lÄ±yor...")
    print("=" * 60)
    
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)